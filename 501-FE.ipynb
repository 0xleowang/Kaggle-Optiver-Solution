{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('gpu': conda)"
  },
  "interpreter": {
   "hash": "92e15e089105a7a863a6daf4d8bb8fefc62afebb9ac515f25b6afacc0274c8c5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import glob\n",
    "import os, gc\n",
    "import numpy as numpy\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import r2_score\n",
    "from numba import njit\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from utils import *\n",
    "from numba_functions import *\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "# TF\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-18 02:31:19.977668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "N_FOLD = 5\n",
    "N_MINS = 5\n",
    "MIN_SIZE = 600 // N_MINS\n",
    "\n",
    "SOL_NAME = '501'\n",
    "mkdir(f'./models/{SOL_NAME}/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# CONSTANT\n",
    "MEAN = -5.762330803300896\n",
    "STD = 0.6339307835941186\n",
    "EPS = 1e-9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# stock and time id\n",
    "list_stock_id = get_stock_id()\n",
    "list_time_id = get_time_id()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature generating functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def unstack_agg(df, agg_col):\n",
    "    df = df.unstack(level=1)\n",
    "    df.columns = [f'{agg_col}_{k}' for k in df.columns]\n",
    "    return df.reset_index()\n",
    "\n",
    "def init_feature_df(df_book, stock_id):\n",
    "    df_feature = pd.DataFrame(df_book['time_id'].unique())\n",
    "    df_feature['stock_id'] = stock_id\n",
    "    df_feature.columns = ['time_id', 'stock_id']\n",
    "    return df_feature[['stock_id', 'time_id']]\n",
    "\n",
    "def add_stats(df, cols, data_name, suffix='', axis=0):\n",
    "    unwrap = lambda x: x.item() if len(cols) == 1 else x\n",
    "    df[f'{data_name}_{suffix}_mean'] = unwrap(df[cols].mean(axis=axis).values)\n",
    "    df[f'{data_name}_{suffix}_std'] = unwrap(df[cols].std(axis=axis).values)\n",
    "    df[f'{data_name}_{suffix}_skew'] = unwrap(df[cols].skew(axis=axis).values)\n",
    "    df[f'{data_name}_{suffix}_min'] = unwrap(df[cols].min(axis=axis).values)\n",
    "    df[f'{data_name}_{suffix}_q1'] = unwrap(df[cols].quantile(q=0.25, axis=axis).values)\n",
    "    df[f'{data_name}_{suffix}_q2'] = unwrap(df[cols].quantile(q=0.50, axis=axis).values)\n",
    "    df[f'{data_name}_{suffix}_q3'] = unwrap(df[cols].quantile(q=0.75, axis=axis).values)\n",
    "    df[f'{data_name}_{suffix}_max'] = unwrap(df[cols].max(axis=axis).values)\n",
    "    return df\n",
    "\n",
    "def add_feature_min(df_feature, df, configs):\n",
    "    df['min_id'] = df['seconds_in_bucket'] // MIN_SIZE\n",
    "    df_gb_min = df.groupby(['time_id', 'min_id'])\n",
    "    for data_col, agg_func, agg_col in configs:\n",
    "        # agg by min\n",
    "        df_ = df_gb_min[data_col].agg(agg_func, engine='numba')\n",
    "        df_ = unstack_agg(df_, agg_col)\n",
    "        df_feature = df_feature.merge(df_, on=['time_id'], how='left')\n",
    "        # gen stats by min and by time\n",
    "        cols = [f'{agg_col}_{k}' for k in range(N_MINS)]\n",
    "        for c in cols:\n",
    "            if c not in df_feature:\n",
    "                df_feature[c] = 0\n",
    "        df_feature = add_stats(df_feature, cols=cols, data_name=agg_col, suffix='min', axis=1)\n",
    "    return df_feature.fillna(0.0)\n",
    "\n",
    "def add_feature_time(df_feature, df, configs):\n",
    "    df_gb_time = df.groupby(['time_id'])\n",
    "    for data_col, agg_func, agg_col in configs:\n",
    "        # agg by time\n",
    "        df_ = df_gb_time[data_col].agg(agg_func, engine='numba')\n",
    "        df_.name = f'{agg_col}_time'\n",
    "        df_feature = df_feature.merge(df_, on=['time_id'], how='left')\n",
    "    return df_feature.fillna(0.0)\n",
    "\n",
    "def ffill_book(df_book):\n",
    "    list_time_id_book = df_book.time_id.unique()\n",
    "    df_ = pd.DataFrame()\n",
    "    df_['time_id'] = np.matlib.repeat(list_time_id_book, 600)\n",
    "    df_['seconds_in_bucket'] = np.matlib.repmat(range(600), 1, len(list_time_id_book)).ravel()\n",
    "    df_book = df_.merge(df_book, on=['time_id', 'seconds_in_bucket'], how='left')\n",
    "    df_book = df_book.set_index('time_id').groupby(level='time_id').ffill().bfill().reset_index() \n",
    "    return df_book"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "book_configs = [\n",
    "    ('log_return1', rv_numba, 'B_RV1'),\n",
    "    ('log_return2', rv_numba, 'B_RV2'),\n",
    "    ('seconds_in_bucket', count_numba, 'B_NROW'),\n",
    "    ('bid_vol1', sum_numba, 'B_BVOL1'),\n",
    "    ('bid_vol2', sum_numba, 'B_BVOL2'),\n",
    "    ('ask_vol1', sum_numba, 'B_AVOL1'),\n",
    "    ('ask_vol2', sum_numba, 'B_AVOL2'),\n",
    "]\n",
    "\n",
    "book_configs_ffill = [\n",
    "    ('bid_price1', mean_numba, 'B_BP1'),\n",
    "    ('bid_price2', mean_numba, 'B_BP2'),\n",
    "    ('ask_price1', mean_numba, 'B_AP1'),\n",
    "    ('ask_price2', mean_numba, 'B_AP2'),\n",
    "    ('bid_size1', mean_numba, 'B_BS1'),\n",
    "    ('bid_size2', mean_numba, 'B_BS2'),\n",
    "    ('ask_size1', mean_numba, 'B_AS1'),\n",
    "    ('ask_size2', mean_numba, 'B_AS2'),\n",
    "    # new features\n",
    "    ('price1_diff', mean_numba, 'Z_P1-DIFF'),\n",
    "    ('price2_diff', mean_numba, 'Z_P2-DIFF'),\n",
    "    ('price1_dabs', mean_numba, 'Z_P1-DABS'),\n",
    "    ('price2_dabs', mean_numba, 'Z_P2-DABS'),\n",
    "    ('price_spread1', mean_numba, 'Z_SPREAD1'),\n",
    "]\n",
    "\n",
    "trade_configs = [\n",
    "    ('vol', sum_numba, 'T_VOL'),\n",
    "    ('order_count', sum_numba, 'T_OC'),\n",
    "    ('size', sum_numba, 'T_SIZE'),\n",
    "    ('seconds_in_bucket', count_numba, 'T_NROW'),\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# [f[-1] for f in book_configs+book_configs_ffill+trade_configs] + ['Z_RATIO']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def gen_df_feature(stock_id):\n",
    "    # -----------------------------------------------------------------\n",
    "    # Book data (no ffill)\n",
    "    book_parquet_path = get_path_by_id('book', stock_id)\n",
    "    df_book = pd.read_parquet(book_parquet_path)\n",
    "    df_book_ff = df_book.copy()\n",
    "    df_feature = init_feature_df(df_book, stock_id)\n",
    "    # add wap and log_return\n",
    "    df_book['wap1'] = calc_wap_njit(\n",
    "        df_book.bid_price1.values,\n",
    "        df_book.ask_price1.values,\n",
    "        df_book.bid_size1.values,\n",
    "        df_book.ask_size1.values\n",
    "    )\n",
    "    df_book['wap2'] = calc_wap_njit(\n",
    "        df_book.bid_price2.values,\n",
    "        df_book.ask_price2.values,\n",
    "        df_book.bid_size1.values + df_book.bid_size2.values,\n",
    "        df_book.ask_size1.values + df_book.ask_size2.values\n",
    "    )\n",
    "    df_book['log_return1'] = df_book.groupby(['time_id'])['wap1'].apply(calc_log_return).fillna(0)\n",
    "    df_book['log_return2'] = df_book.groupby(['time_id'])['wap2'].apply(calc_log_return).fillna(0)\n",
    "    # add vols\n",
    "    df_book['bid_vol1'] = prod_njit(df_book['bid_price1'].values, df_book['bid_size1'].values)\n",
    "    df_book['bid_vol2'] = prod_njit(df_book['bid_price2'].values, df_book['bid_size2'].values)\n",
    "    df_book['ask_vol1'] = prod_njit(df_book['ask_price1'].values, df_book['ask_size1'].values)\n",
    "    df_book['ask_vol2'] = prod_njit(df_book['ask_price2'].values, df_book['ask_size2'].values)\n",
    "    # generate book features\n",
    "    df_feature = add_feature_min(df_feature, df_book, book_configs)\n",
    "    df_feature = add_feature_time(df_feature, df_book, book_configs)\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Book data (ffill) \n",
    "    df_book_ff = ffill_book(df_book_ff)\n",
    "    # new features\n",
    "    df_book_ff['price1_diff'] = df_book_ff['ask_price1'] - df_book_ff['bid_price1']\n",
    "    df_book_ff['price2_diff'] = df_book_ff['ask_price2'] - df_book_ff['bid_price2']\n",
    "    df_book_ff['price1_dabs'] = df_book_ff['price1_diff'].abs()\n",
    "    df_book_ff['price2_dabs'] = df_book_ff['price2_diff'].abs()\n",
    "    df_book_ff['price_spread1'] = (df_book_ff['ask_price1'] - df_book_ff['bid_price1']) / (df_book_ff['ask_price1'] + df_book_ff['bid_price1'])\n",
    "    # generate book features\n",
    "    df_feature = add_feature_min(df_feature, df_book_ff, book_configs_ffill)\n",
    "    df_feature = add_feature_time(df_feature, df_book_ff, book_configs_ffill)\n",
    "    \n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Trade data\n",
    "    trade_parquet_path = get_path_by_id('trade', stock_id)\n",
    "    df_trade = pd.read_parquet(trade_parquet_path)\n",
    "    # add vol\n",
    "    df_trade['vol'] = prod_njit(df_trade['price'].values, df_trade['size'].values)\n",
    "    # generate trade features\n",
    "    df_feature = add_feature_min(df_feature, df_trade, trade_configs)\n",
    "    df_feature = add_feature_time(df_feature, df_trade, trade_configs)\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Combined feature\n",
    "    log_return = df_trade.merge(df_book, on=['time_id', 'seconds_in_bucket'], how='left').groupby('time_id')['log_return1'].agg(lambda x: np.sum(np.square(x)))\n",
    "    total_log_return = df_book.groupby('time_id')['log_return1'].agg(lambda x: np.sum(np.square(x)))\n",
    "    df_feature['Z_RATIO'] = (log_return / total_log_return).values\n",
    "    df_feature['Z_RATIO'] = df_feature['Z_RATIO'].fillna(0.0)\n",
    "    return df_feature"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "list_dfs = Parallel(n_jobs=-1)(delayed(gen_df_feature)(stock_id) for stock_id in tqdm(list_stock_id))\n",
    "df_train = pd.concat(list_dfs).reset_index(drop=True)\n",
    "df_train = df_train.sort_values(['stock_id', 'time_id']).reset_index(drop=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 112/112 [04:00<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Combine feature and target\n",
    "df_target = dt.fread('./dataset/train.csv').to_pandas()\n",
    "df_train = df_train.merge(df_target, on=['stock_id', 'time_id'], how='inner', validate='one_to_one')\n",
    "fea_cols = [c for c in df_train.columns if c.startswith('B_') or c.startswith('T_') or c.startswith('Z_')]\n",
    "# Save df_train\n",
    "dt.Frame(df_train).to_csv(f'./dataset/train_{SOL_NAME}_LGB.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df_train = dt.fread(f'./dataset/train_{SOL_NAME}_LGB.csv').to_pandas()\n",
    "fea_cols = [c for c in df_train.columns if c.startswith('B_') or c.startswith('T_') or c.startswith('Z_')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pipe = make_pipeline(StandardScaler(), PowerTransformer())\n",
    "df_train[fea_cols] = pipe.fit_transform(df_train[fea_cols].values)\n",
    "save_pickle(pipe, f'./models/{SOL_NAME}/pipe.pkl')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# df_train[fea_cols] = df_train[fea_cols].clip(-10, 10, axis=1)\n",
    "dt.Frame(df_train).to_csv(f'./dataset/train_{SOL_NAME}_NN_noclip.csv')\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}